<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fileupload on Brijesh Pant</title>
    <link>brijeshpant.github.io/tags/fileupload.html</link>
    <description>Recent content in Fileupload on Brijesh Pant</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>pantbrijesh20@gmail.com (brijesh pant)</managingEditor>
    <webMaster>pantbrijesh20@gmail.com (brijesh pant)</webMaster>
    <lastBuildDate>Sat, 30 Jan 2016 15:16:28 +0530</lastBuildDate>
    <atom:link href="brijeshpant.github.io/tags/fileupload.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Progressively Uploading Csv using Play</title>
      <link>/brijeshpant.github.io/2016/01/progressivelyuploadingcsv/</link>
      <pubDate>Sat, 30 Jan 2016 15:16:28 +0530</pubDate>
      <author>pantbrijesh20@gmail.com (brijesh pant)</author>
      <guid>/brijeshpant.github.io/2016/01/progressivelyuploadingcsv/</guid>
      <description>

&lt;h1 id=&#34;progressively-uploading-csv:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Progressively Uploading CSV&lt;/h1&gt;

&lt;h1 id=&#34;scenario:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Scenario&lt;/h1&gt;

&lt;p&gt;You may have encountered situations, when you need to send huge amount of data (in GBs ) to the server.  One example is upload of large files. While sending this data, your server has to slog through the upcoming mighty request body and eventually process it.&lt;/p&gt;

&lt;h1 id=&#34;problem-definition:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Problem Definition&lt;/h1&gt;

&lt;p&gt;So this whole process is about getting chunk of data, wait for next chunk to come and then wait again and this goes on until it receives whole request body.  All the request body data is now in servers memory. And multiple simultaneous uploads would invariably lead to a OOM error.&lt;/p&gt;

&lt;p&gt;Also till the request body is saved to disk the file/request data resides in memory and blocks the thread. As you would imagine both memory and threads are precious and scant resources.&lt;/p&gt;

&lt;p&gt;In such conditions, what can you expect but the frustratingly slow response? Would it not be nice if there is a way to handle such requests in a smarter way?&lt;/p&gt;

&lt;h1 id=&#34;solution:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Solution&lt;/h1&gt;

&lt;p&gt;As the browser sends data in chunks of byte over the network, processing the incoming data in chunks would be a smarter choice. Play2 framework provides such a beautiful capability in the form  of Iteratee library. Iteratees support the consumption of data in chunks and in an asynchronous manner. For more detail of Iteratee you can browse through Iteratee.&lt;/p&gt;

&lt;h1 id=&#34;iteratee:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Iteratee&lt;/h1&gt;

&lt;p&gt;For now you need to understand what Iteratee can do to solve our problem. Iteratee requires producer that can feed iteratee the data to process. Producer can be any Enumerator which has capability to produce data of same type as the Iteratee expects. Iteratee can consume the chunks of data progressively. It does not need all data to be radially available. It can consume data chunks whenever it is available. Iteratee doses not wait on any upcoming chunks. It is the responsibility of producer to feed Iteratee. After receiving data chunk Iteratee can start processing it.&lt;/p&gt;

&lt;p&gt;Here we will see the example code for uploading the csv file progressively. This is a Play-scala application so you need to set the required environment. Once you done with creating a play application, you are ready to go further. Here are instructions on   &lt;a href=&#34;https://www.playframework.com/documentation/2.2.x/Installing&#34;&gt;how to install and setup play&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;code:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Code&lt;/h1&gt;

&lt;p&gt;Make entries for your routes in Routes file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET / controllers.Application.index //For opening home page
POST /upload controllers.Application.upload //Post request to upload file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create scala html template index.scala.html .&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@helper.form(action = routes.Application.upload, &#39;enctype -&amp;gt; &amp;quot;multipart/form-data&amp;quot;) { Please upload file
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add following methods in Application controller to put your handler for requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def index = Action {
Ok(views.html.index())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;index method just upon up your home page from where CSV file can be uploaded.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def upload = Action(BodyParser(request =&amp;gt; { CsvBodyParser.parseCsvData(false)})) {
rq: Request[List[String]] =&amp;gt;
Ok(&amp;quot;file uploaded successfully&amp;quot;)
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The upload method, handles the post request containing the body. You can see BodyParser is used explicitly with the Action. By default all Action takes play.api.mvc.AnyContent as BodyParser to parse the request to some Scala value.it adapts automatically according to the request Content-Type. BodyParser[A] is basically an Iteratee[Array[Byte],A] which consumes the chunk of data as Array[Byte] as long as browser sends them and return the type A which is passed into the Request to be processed by Action. Here is the helper class CsvBodyParser which parses the request and return List[String]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package controllers
import play.api.libs.iteratee.{Iteratee, Enumeratee, Parsing}
import scala.List
import scala.Predef._
import scala.Predef.String
import play.api.Logger

object CsvBodyParser {
	var flag = false
	var headerList: List[String] = null

	def parseCsvData(f: Boolean) = {
		headerList = null
		flag = f
		val seperator = &amp;quot;,&amp;quot;
		Parsing.search(&amp;quot;n&amp;quot;.getBytes) &amp;gt;&amp;lt;&amp;gt; Enumeratee.grouped(
			(Enumeratee.breakE[Parsing.MatchInfo[Array[Byte]]] (_.isMatch)  &amp;gt;&amp;lt;&amp;gt;
				Enumeratee.collect {
					case Parsing.Unmatched(bytes) =&amp;gt;
					val stringVal =new String(bytes)
					stringVal
				}
				&amp;amp;&amp;gt;&amp;gt;
				Iteratee.consume()).flatMap(r =&amp;gt; {
					if (flag) {
						Iteratee.head.map(_ =&amp;gt; processLine(r.trim.split(seperator)))
						} else {
							val list = r.toString.trim.split(seperator).toList
							if (list.length &amp;gt; 0 &amp;amp;&amp;amp; list.head.equals(&amp;quot;name&amp;quot;)) {
								headerList = list
								flag = true
							}
							Iteratee.head.map(_ =&amp;gt; &amp;quot;&amp;quot;)
						}
						})
				) &amp;amp;&amp;gt;&amp;gt; Iteratee.getChunks.map(Right(_))
	}

	def processLine(line: Array[String]): String = {
		var dataList = line.toList
		var msg = &amp;quot;&amp;quot;
		if (dataList.length &amp;gt;= 1) {
			var dataMap = headerList.toList.zip(dataList).toMap
// dataMap map containing the header and corresponding column for the current row
// You can write your logic to add this into database or what ever you want
//msg =&amp;quot;any success message you can pass&amp;quot;
}
msg
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;explanation:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Explanation&lt;/h1&gt;

&lt;p&gt;In our case as we are parsing a csv file, every row is separated by a new line character. Therefore we need chunks based on the new line to distinguish each line separately and process it. But browsers creates the chunks by its own irrespective of what the separator is.&lt;/p&gt;

&lt;p&gt;Our requirement is to get such an Iteratee which can consume each line at one go and process it. Our source of data here is chunks of Array[Byte] which may not contain new line char or may be containing more than one new lines. Therefore it has to be adapted such that it can match the type our Iteratee is expecting.&lt;/p&gt;

&lt;h1 id=&#34;enumeratee:f188ebac3656b6eb764b262b3dd48a8d&#34;&gt;Enumeratee&lt;/h1&gt;

&lt;p&gt;To adapt one Type of Enumerator to the another type we have Enumeratee which is pipe adapter between Enumerator and Iteratee. It basically transform one Type of Enumerator to other type. For example Enumerator[Int] can be transformed into Enumerator[String] by applying Enumeratee[Int,String] on Enumerator[Int]. Enumeratee[A,B] can also be composed with an Enumeratee[B,C] to give Enumeratee[A,C].&lt;/p&gt;

&lt;p&gt;In the parseCsvData method above you can see we have used Parsing.search(“n”.getBytes) .It searches for new line and gives an Enumeratee[Array[Byte], Parsing.MatchInfo[Array[Byte]]]. then  We compose it with Enumeratee.grouped to regroup on new line. By applying (Enumeratee.breakE&lt;a href=&#34;_.isMatch&#34;&gt;Parsing.MatchInfo[Array[Byte]]&lt;/a&gt;, which is again an adapter that breaks on new line.&lt;/p&gt;

&lt;p&gt;It Gives a new Enumertee which pushes everything it has on to the Iteratee by applying Iteratee.consume[Array[Byte]] which consumes and concatenates all Input chunks and return a Promise.Finally  by applying flatMap we can get actual data for a row.&lt;/p&gt;

&lt;p&gt;Iteratee.head creates an Iteratee that takes the first element of the stream. This element can be anything like header of the csv (first line) ,data or any other information associated with the body. Each column in the csv is separated by comma so we can split it to get the Array. By checking the size we can identify the header. As soon as we get the header we set the flag to true to identify every subsequent row as a data. Now we can process each line, as we have done in the processLine method by zipping the header and column to generate key value pair of header and column. You can do anything with this data to serve your purpose.&lt;/p&gt;

&lt;p&gt;With a little bit of understanding of Play and Iteratees we can now asynchronously upload large bytes of data without stretching the memory requirements.&lt;/p&gt;

&lt;p&gt;Inspired by: &lt;a href=&#34;https://gist.github.com/sadache/2939230&#34;&gt;https://gist.github.com/sadache/2939230&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>